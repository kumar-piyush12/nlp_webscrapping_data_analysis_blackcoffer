{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10fdaac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\91977\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#1 Sentimental Analysis\n",
    "\n",
    "#1.1. Importing articles and saving them as .txt\n",
    "import pandas as pd\n",
    "df = pd.read_csv('url.csv')\n",
    "\n",
    "#Importing Data from multiple url (Class type 1)\n",
    "def give(url,index):\n",
    " from bs4 import BeautifulSoup\n",
    " import requests\n",
    "\n",
    " website = url\n",
    " result = requests.get(website)\n",
    " content = result.text\n",
    " soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    " title = soup.find('h1', class_='entry-title').get_text()\n",
    " # textbox = soup.find('div', class_='td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')\n",
    " innerbox = soup.find('div', class_='td-post-content').get_text(strip=True, separator=' ')\n",
    " full_article = title + innerbox\n",
    " \n",
    " filename = str(index) + '.txt'\n",
    " \n",
    " with open(filename, 'w', encoding='utf-8') as file:\n",
    "  file.write(full_article)\n",
    "\n",
    "#Importing Data from multiple url (Class type 2)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('url.csv')\n",
    "\n",
    "def giver(url,index):\n",
    " from bs4 import BeautifulSoup\n",
    " import requests\n",
    "\n",
    " website = url\n",
    " result = requests.get(website)\n",
    " content = result.text\n",
    " soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    " title = soup.find('h1', class_='tdb-title-text').get_text()\n",
    "#     textbox = soup.find('div', class_='td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')\n",
    " innerbox = soup.find('div', class_='td-post-content').get_text(strip=True, separator=' ')\n",
    " full_article = title + innerbox\n",
    " \n",
    " filename = str(index) + '.txt'\n",
    " \n",
    " with open(filename, 'w', encoding='utf-8') as file:\n",
    "  file.write(full_article)\n",
    "\n",
    "#Importing Data from multiple url (Class type 3)\n",
    "def gave(url,index):\n",
    " from bs4 import BeautifulSoup\n",
    " import requests\n",
    "\n",
    " website = url\n",
    " result = requests.get(website)\n",
    " content = result.text\n",
    " soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    " title = soup.find('h1', class_='entry-title').get_text()\n",
    " # textbox = soup.find('div', class_='td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')\n",
    " innerbox = soup.find('div', class_='td-post-content').get_text(strip=True, separator=' ')\n",
    " full_article = title + innerbox\n",
    " \n",
    " filename = str(index) + '.txt'\n",
    " \n",
    " with open(filename, 'w', encoding='utf-8') as file:\n",
    "  file.write(full_article)\n",
    "\n",
    "#Using a loop to apply all 3 functions for the files\n",
    "functions = [give, giver, gave]\n",
    "\n",
    "i = 0\n",
    "for element in df['url_id']:\n",
    "    url = df['url'][i]\n",
    "    index = element\n",
    "    for func in functions:\n",
    "        try:\n",
    "            func(url, index)  \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred for {func.__name__}: {e}\")\n",
    "            continue\n",
    "    i = i + 1  \n",
    "\n",
    "#7 Personal Pronouns calculation and saving in Output Data Structure.csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stpwrd = nltk.corpus.stopwords.words('english')\n",
    "stpwrd.remove('i')\n",
    "stpwrd.remove('we')\n",
    "stpwrd.remove('my')\n",
    "stpwrd.remove('ours')\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "  try:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "      text = file.read()\n",
    "  except UnicodeDecodeError:\n",
    "    # If 'utf-8-sig' fails, try with 'latin-1' encoding\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "      text = file.read()\n",
    "  return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "  words = nltk.word_tokenize(text)\n",
    "  return words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "  filtered_words = [word for word in words if word.lower() not in stpwrd]\n",
    "  return filtered_words\n",
    "\n",
    "prefix = list(range(37, 151))\n",
    "original_list = prefix\n",
    "prefix_list = [str(element) for element in original_list]\n",
    "original_list = prefix_list\n",
    "suffix = \".txt\"\n",
    "\n",
    "def add_suffix_to_element(element):\n",
    "    return element + suffix\n",
    "\n",
    "filename_list = list(map(add_suffix_to_element, original_list))\n",
    "\n",
    "filename_list.remove('44.txt')\n",
    "filename_list.remove('57.txt')\n",
    "filename_list.remove('144.txt')\n",
    "\n",
    "import re\n",
    "\n",
    "def count_personal_pronouns(word_list):\n",
    "    personal_pronouns = ['I', 'we', 'We', 'my', 'My', 'ours', 'Ours', 'us', 'Us']\n",
    "    word_list_lower = [word.lower() for word in word_list]\n",
    "    pronoun_pattern = r'\\b(?:' + '|'.join(personal_pronouns) + r')\\b'\n",
    "    paragraph_text = ' '.join(word_list_lower)\n",
    "    pronoun_occurrences = re.findall(pronoun_pattern, paragraph_text)\n",
    "    pronoun_occurrences = re.findall(pronoun_pattern, paragraph_text)\n",
    "    pronoun_count = len(pronoun_occurrences)\n",
    "    return pronoun_count\n",
    "\n",
    "personal_pronoun_all_files = []\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    filtered_words = remove_stopwords(words)\n",
    "    x = count_personal_pronouns(filtered_words)\n",
    "    personal_pronoun_all_files.append(x)\n",
    "\n",
    "def write_list_to_csv_column(data_list, column_name, csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[column_name] = data_list\n",
    "    df.to_csv(csv_file, index=False)    \n",
    "\n",
    "write_list_to_csv_column(personal_pronoun_all_files, 'PERSONAL PRONOUNS', 'Output Data Structure.csv')\n",
    "\n",
    "#1.1. Modifying Stopwords and Cleaning all the articles\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def import_txt_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            words = content.split()  # Split the content into separate words using whitespace as the delimiter\n",
    "            return words\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the file path.\")\n",
    "        return []\n",
    "\n",
    "file_path = 'StopWords_Auditor.txt'\n",
    "words_list = import_txt_file(file_path)\n",
    "words_list_1 = words_list\n",
    "\n",
    "file_path = 'StopWords_Currencies.txt'\n",
    "words_list_2 = import_txt_file(file_path)\n",
    "\n",
    "file_path = 'StopWords_DatesandNumbers.txt'\n",
    "words_list_3 = import_txt_file(file_path)\n",
    "\n",
    "file_path = 'StopWords_Generic.txt'\n",
    "words_list_4 = import_txt_file(file_path)\n",
    "\n",
    "file_path = 'StopWords_GenericLong.txt'\n",
    "words_list_5 = import_txt_file(file_path)\n",
    "\n",
    "file_path = 'StopWords_Geographic.txt'\n",
    "words_list_6 = import_txt_file(file_path)\n",
    "\n",
    "file_path = 'StopWords_Names.txt'\n",
    "words_list_7 = import_txt_file(file_path)\n",
    "\n",
    "words_addition = words_list_1 + words_list_2 + words_list_3 + words_list_4 + words_list_5 + words_list_6 + words_list_7\n",
    "\n",
    "added_words = set(words_addition)\n",
    "\n",
    "stpwrd = nltk.corpus.stopwords.words('english')\n",
    "stpwrd.extend(added_words)\n",
    "\n",
    "final_stopwords = list(map(str.lower, stpwrd))\n",
    "final_set_stopwords = set(final_stopwords)\n",
    "\n",
    "  #1 Reading content of txt file\n",
    "def read_txt_file(file_path):\n",
    "  try:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "      text = file.read()\n",
    "  except UnicodeDecodeError:\n",
    "    # If 'utf-8-sig' fails, try with 'latin-1' encoding\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "      text = file.read()\n",
    "  return text\n",
    "\n",
    "  #2 Tokenizing the text\n",
    "def tokenize_text(text):\n",
    "  words = nltk.word_tokenize(text)\n",
    "  return words\n",
    "\n",
    "  #3 Removing stopwords\n",
    "def remove_stopwords(words):\n",
    "  filtered_words = [word for word in words if word.lower() not in final_stopwords]\n",
    "  return filtered_words\n",
    "\n",
    "  #4 Saving a new file\n",
    "def write_filtered_text(file_path, filtered_text):\n",
    "  with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(' '.join(filtered_text))\n",
    "\n",
    "  # Making a file reader for loop\n",
    "import numpy as np\n",
    "prefix = list(range(37, 151))\n",
    "\n",
    "original_list = prefix\n",
    "prefix_list = [str(element) for element in original_list]\n",
    "# print(prefix_list)\n",
    "\n",
    "original_list = prefix_list\n",
    "suffix = \".txt\"\n",
    "\n",
    "def add_suffix_to_element(element):\n",
    "    return element + suffix\n",
    "\n",
    "filename_list = list(map(add_suffix_to_element, original_list))\n",
    "# print(filename_list)\n",
    "\n",
    "filename_list.remove('44.txt')\n",
    "filename_list.remove('57.txt')\n",
    "filename_list.remove('144.txt')\n",
    "\n",
    "for filename in filename_list:\n",
    "  text = read_txt_file(filename)\n",
    "  words = tokenize_text(text)\n",
    "  filtered_words = remove_stopwords(words)\n",
    "  write_filtered_text(filename, filtered_words)\n",
    "\n",
    "#1.2 Creating Positive and Negative Dictionary\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = read_txt_file('negative-words.txt')\n",
    "words = tokenize_text(text)\n",
    "filtered_words = remove_stopwords(words)\n",
    "write_filtered_text('new-negative-words.txt', filtered_words)\n",
    "\n",
    "text = read_txt_file('positive-words.txt')\n",
    "words = tokenize_text(text)\n",
    "filtered_words = remove_stopwords(words)\n",
    "write_filtered_text('new-positive-words.txt', filtered_words)\n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "text = read_txt_file('new-positive-words.txt')\n",
    "positive_words = tokenize_text(text)\n",
    "def create_custom_positive_dictionary():\n",
    "    positive_dictionary = positive_words\n",
    "    return set(positive_dictionary)\n",
    "positive_dictionary = create_custom_positive_dictionary()\n",
    "\n",
    "text = read_txt_file('new-negative-words.txt')\n",
    "negative_words = tokenize_text(text)\n",
    "def create_custom_negative_dictionary():\n",
    "    negative_dictionary = negative_words\n",
    "    return set(negative_dictionary)\n",
    "negative_dictionary = create_custom_negative_dictionary()\n",
    "\n",
    "positive_list = list(positive_dictionary) #List containing positive words\n",
    "negative_list = list(negative_dictionary) #List containing negative words\n",
    "\n",
    "#1.3.1. Calculating and Extracting Positive and Negative Score in Output Data Structure\n",
    "# Making a file reader for loop\n",
    "import numpy as np\n",
    "prefix = list(range(37, 151))\n",
    "\n",
    "def find_common_elements(list1, list2):\n",
    "    common_elements = [element for element in list1 if element in list2]\n",
    "    return common_elements\n",
    "\n",
    "common_positive_elements = find_common_elements(words, positive_list)\n",
    "common_negative_elements = find_common_elements(words, negative_list)\n",
    "\n",
    "original_list = prefix\n",
    "prefix_list = [str(element) for element in original_list]\n",
    "# print(prefix_list)\n",
    "\n",
    "original_list = prefix_list\n",
    "suffix = \".txt\"\n",
    "\n",
    "def add_suffix_to_element(element):\n",
    "    return element + suffix\n",
    "\n",
    "filename_list = list(map(add_suffix_to_element, original_list))\n",
    "# print(filename_list)\n",
    "\n",
    "filename_list.remove('44.txt')\n",
    "filename_list.remove('57.txt')\n",
    "filename_list.remove('144.txt')\n",
    "\n",
    "# for filename in filename_list:\n",
    "#     print(filename)\n",
    "count_positive_words = []\n",
    "count_negative_words = []\n",
    "\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    common_positive_elements = find_common_elements(words, positive_list)\n",
    "    x = len(common_positive_elements)\n",
    "    count_positive_words.append(x)\n",
    "    \n",
    "    common_negative_elements = find_common_elements(words, negative_list)\n",
    "    y = len(common_negative_elements)\n",
    "    count_negative_words.append(y)\n",
    "\n",
    "    #len(count_negative_words)=111 & same for positive (111 files)\n",
    "    \n",
    "output_csv = pd.read_csv('Output Data Structure.csv')\n",
    "\n",
    "def write_list_to_csv_column(data_list, column_name, csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[column_name] = data_list\n",
    "    df.to_csv(csv_file, index=False)   \n",
    "    \n",
    "write_list_to_csv_column(count_positive_words, 'POSITIVE SCORE', 'Output Data Structure.csv')\n",
    "write_list_to_csv_column(count_negative_words, 'NEGATIVE SCORE', 'Output Data Structure.csv')\n",
    "\n",
    "\n",
    "#1.3.2. Calculating and Saving Polarity Index\n",
    "\n",
    "def polarity_score_calculator(x,y):\n",
    "  #x is Positive score and y is Negative score\n",
    "    polarity_score = (0.000001) + ((x-y)/(x+y))\n",
    "    return polarity_score\n",
    "df = pd.read_csv('Output Data Structure.csv')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    positive_score = row['POSITIVE SCORE']\n",
    "    negative_score = row['NEGATIVE SCORE']\n",
    "    result = polarity_score_calculator(positive_score, negative_score)\n",
    "    df.at[index, 'POLARITY SCORE'] = result\n",
    "    df.to_csv('Output Data Structure.csv', index=False) #File overwritten\n",
    "\n",
    "\n",
    "#1.3.3. Calculating and Saving Subjectivity Index\n",
    "import numpy as np\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "  try:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "      text = file.read()\n",
    "  except UnicodeDecodeError:\n",
    "    # If 'utf-8-sig' fails, try with 'latin-1' encoding\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "      text = file.read()\n",
    "  return text\n",
    "\n",
    "  #2 Tokenizing the text\n",
    "def tokenize_text(text):\n",
    "  words = nltk.word_tokenize(text)\n",
    "  return words\n",
    "\n",
    "  #Loop to read the files\n",
    "prefix = list(range(37, 151))\n",
    "original_list = prefix\n",
    "prefix_list = [str(element) for element in original_list]\n",
    "original_list = prefix_list\n",
    "suffix = \".txt\"\n",
    "\n",
    "def add_suffix_to_element(element):\n",
    "    return element + suffix\n",
    "\n",
    "filename_list = list(map(add_suffix_to_element, original_list))\n",
    "\n",
    "filename_list.remove('44.txt')\n",
    "filename_list.remove('57.txt')\n",
    "filename_list.remove('144.txt')\n",
    "\n",
    "  #Function of subjectivity_score\n",
    "def subjectivity_score(positive_score, negative_score, total_words_after_cleaning):\n",
    "    x = positive_score\n",
    "    y = negative_score\n",
    "    z = total_words_after_cleaning\n",
    "    subjectivity_score = (x+y)/(z+0.000001)\n",
    "    return subjectivity_score\n",
    "\n",
    "  #Executing the loop to fill the values and save it\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    total_words_after_cleaning = len(words)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        positive_score = row['POSITIVE SCORE']\n",
    "        negative_score = row['NEGATIVE SCORE']\n",
    "        result = subjectivity_score(positive_score, negative_score, total_words_after_cleaning)\n",
    "        df.at[index, 'SUBJECTIVITY SCORE'] = result\n",
    "        df.to_csv('Output Data Structure.csv', index=False) #File overwritten\n",
    "\n",
    "#5 Wordcount\n",
    "  #Element Finder in stopwords or a list\n",
    "  # element_to_find = 'you'\n",
    "\n",
    "  # try:\n",
    "  #     index = stopwords_for_word_count.index(element_to_find)\n",
    "  #     print(f\"Element found at index: {index}\")\n",
    "  # except ValueError:\n",
    "  #     print(\"Element not found in the list.\")\n",
    "punctuations_list = ['“','”','’','.',',',',','(',')','?','{','}','-','/',':',';']\n",
    "nltk_stpwrd = nltk.corpus.stopwords.words('english')\n",
    "stopwords_for_word_count = punctuations_list + nltk_stpwrd\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "  try:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "      text = file.read()\n",
    "  except UnicodeDecodeError:\n",
    "    # If 'utf-8-sig' fails, try with 'latin-1' encoding\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "      text = file.read()\n",
    "  return text\n",
    "\n",
    "  #2 Tokenizing the text\n",
    "def tokenize_text(text):\n",
    "  words = nltk.word_tokenize(text)\n",
    "  return words\n",
    "\n",
    "  #3 Removing stopwords\n",
    "def remove_stopwords(words):\n",
    "  filtered_words = [word for word in words if word.lower() not in stopwords_for_word_count]\n",
    "  return filtered_words\n",
    "\n",
    "word_count_list = []\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    filtered_words = remove_stopwords(words)\n",
    "    word_count = len(filtered_words)\n",
    "    word_count_list.append(word_count)\n",
    "\n",
    "def write_list_to_csv_column(data_list, column_name, csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[column_name] = data_list\n",
    "    df.to_csv(csv_file, index=False)   \n",
    "\n",
    "write_list_to_csv_column(word_count_list, 'WORD COUNT', 'Output Data Structure.csv')\n",
    "\n",
    "\n",
    "#8 Average Word Length calculation and storing in csv\n",
    "punctuations_list = ['?','!',',','.','\"','(',')','-','“','”','’',':','%','@','#','$','^','&','*','<','>','/','{','}']\n",
    "nltk_stpwrd = nltk.corpus.stopwords.words('english')\n",
    "stopwords_for_word_count = punctuations_list + nltk_stpwrd\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "  try:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "      text = file.read()\n",
    "  except UnicodeDecodeError:\n",
    "    # If 'utf-8-sig' fails, try with 'latin-1' encoding\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "      text = file.read()\n",
    "  return text\n",
    "\n",
    "  #2 Tokenizing the text\n",
    "def tokenize_text(text):\n",
    "  words = nltk.word_tokenize(text)\n",
    "  return words\n",
    "\n",
    "  #3 Removing stopwords\n",
    "def remove_stopwords(words):\n",
    "  filtered_words = [word for word in words if word.lower() not in stopwords_for_word_count]\n",
    "  return filtered_words\n",
    "\n",
    "prefix = list(range(37, 151))\n",
    "original_list = prefix\n",
    "prefix_list = [str(element) for element in original_list]\n",
    "original_list = prefix_list\n",
    "suffix = \".txt\"\n",
    "\n",
    "def add_suffix_to_element(element):\n",
    "    return element + suffix\n",
    "\n",
    "filename_list = list(map(add_suffix_to_element, original_list))\n",
    "\n",
    "filename_list.remove('44.txt')\n",
    "filename_list.remove('57.txt')\n",
    "filename_list.remove('144.txt')\n",
    "\n",
    "#Using Loop to count number of words in each file and storing in a list\n",
    "word_count_list = []\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    filtered_words = remove_stopwords(words)\n",
    "    word_count = len(filtered_words)\n",
    "    word_count_list.append(word_count)\n",
    "\n",
    "#Using Loop to count number of characters in each file and storing in a list\n",
    "character_count_list = []\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    filtered_words = remove_stopwords(words)\n",
    "    character_count_individual = [len(text_item) for text_item in filtered_words]\n",
    "    character_count = sum(character_count_individual)\n",
    "    character_count_list.append(character_count) \n",
    "    \n",
    "def write_list_to_csv_column(data_list, column_name, csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[column_name] = data_list\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    \n",
    "# Perform element-wise division using list comprehension\n",
    "avg_word_length_list = [a / b for a, b in zip(character_count_list, word_count_list)]\n",
    "write_list_to_csv_column(avg_word_length_list, 'AVG WORD LENGTH', 'Output Data Structure.csv')\n",
    "\n",
    "\n",
    "#2 & 3: Avg sentence length or Avg number of words per sentence\n",
    "word_count_list = []\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    filtered_words = remove_stopwords(words)\n",
    "    word_count = len(filtered_words)\n",
    "    word_count_list.append(word_count)\n",
    "\n",
    "sentence_count_list = []\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    sentence_count_list.append(num_sentences)\n",
    "\n",
    "avg_word_length_list = [a / b for a, b in zip(word_count_list, sentence_count_list)]\n",
    "write_list_to_csv_column(avg_word_length_list, 'AVG SENTENCE LENGTH', 'Output Data Structure.csv')\n",
    "write_list_to_csv_column(avg_word_length_list, 'AVG NUMBER OF WORDS PER SENTENCE', 'Output Data Structure.csv')\n",
    "\n",
    "#6 Syllable count per word (total syllables of a file / total word count of a file)    \n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "list_syllable_count_all_files = []\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    filtered_words = remove_stopwords(words)\n",
    "    list_syllable_count_individual_file = []\n",
    "    for element in filtered_words:\n",
    "        x = syllable_count(element)\n",
    "        list_syllable_count_individual_file.append(x)\n",
    "    y = sum(list_syllable_count_individual_file)\n",
    "    list_syllable_count_all_files.append(y)\n",
    "\n",
    "syllable_per_word = [a / b for a, b in zip(list_syllable_count_all_files, word_count_list)]\n",
    "write_list_to_csv_column(syllable_per_word, 'SYLLABLE PER WORD', 'Output Data Structure.csv')\n",
    "\n",
    "\n",
    "#4 Complex Word Count of individual files and saving them in Output\n",
    "complex_word_count_all_files_list = []\n",
    "for filename in filename_list:\n",
    "    text = read_txt_file(filename)\n",
    "    words = tokenize_text(text)\n",
    "    filtered_words = remove_stopwords(words)\n",
    "    complex_word_count_individual_file = 0\n",
    "    for element in filtered_words:\n",
    "        x = syllable_count(element)\n",
    "        if x > 2:\n",
    "            complex_word_count_individual_file += 1\n",
    "        else:\n",
    "            complex_word_count_individual_file = complex_word_count_individual_file\n",
    "    complex_word_count_all_files_list.append(complex_word_count_individual_file)\n",
    "    \n",
    "write_list_to_csv_column(complex_word_count_all_files_list, 'COMPLEX WORD COUNT', 'Output Data Structure.csv')\n",
    "\n",
    "#2.2. Percentage Complex Words and storing them in Output Data Structure.csv\n",
    "complex_word_count_all_files_list\n",
    "word_count_list\n",
    "\n",
    "complex_words_fraction_all_files = [a / b for a, b in zip(complex_word_count_all_files_list, word_count_list)]\n",
    "\n",
    "def multiply_list_elements_with_constant(lst, constant):\n",
    "    result_list = []\n",
    "    for element in lst:\n",
    "        result_list.append(element * constant)\n",
    "    return result_list\n",
    "\n",
    "multiply_list_elements_with_constant(complex_words_fraction_all_files, 100)\n",
    "\n",
    "complex_words_percentage_all_files = multiply_list_elements_with_constant(complex_words_fraction_all_files, 100)\n",
    "\n",
    "write_list_to_csv_column(complex_words_percentage_all_files, 'PERCENTAGE OF COMPLEX WORDS', 'Output Data Structure.csv')\n",
    "\n",
    "\n",
    "#2.3. Gunning Fog Index for each file and saving as output in Output Data Structure.csv\n",
    "def fog_index(avg_sentence_length, percentage_complex_words):\n",
    "    x = avg_sentence_length\n",
    "    y = percentage_complex_words\n",
    "    fog_index = 0.4 * (x+y)\n",
    "    return fog_index    \n",
    "\n",
    "avg_word_length_list\n",
    "complex_words_percentage_all_files\n",
    "\n",
    "gunning_fog_index_all_files = []\n",
    "i = 0\n",
    "for x in avg_word_length_list:\n",
    "    y = complex_words_percentage_all_files[i]\n",
    "    z = fog_index(x,y)\n",
    "    gunning_fog_index_all_files.append(z)\n",
    "    i+=1\n",
    "    \n",
    "write_list_to_csv_column(gunning_fog_index_all_files, 'FOG INDEX', 'Output Data Structure.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd540f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(personal_pronoun_all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642150ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48062971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
